{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27372111-4c7d-4cb9-8960-01cff85aa843",
   "metadata": {},
   "source": [
    "# Fish track visualistion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af151e-2ee9-4233-89fd-4499c981b860",
   "metadata": {},
   "source": [
    "___\n",
    "### This notebooks is used to plot a panel dashboard to visualize data for the GFTS project\n",
    "____\n",
    "The [panel dashboard](https://panel.holoviz.org/) displays the following informations :\n",
    "- The temperature measured by the fish.\n",
    "- The depth measured by the fish.\n",
    "- The evolution of the latitude and the longitude during the time.\n",
    "___\n",
    "First of all, you need to access the following [jubyterhub server](https://gfts.minrk.net), you will need to log with your github account and you will select a configuration for starting a notebook. \n",
    "\n",
    "To acces this notebook you have to clone the following [github repository](https://github.com/destination-earth/DestinE_ESA_GFTS). \n",
    "First you will fork the repository, see this [link](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo). \n",
    "Then, you will clone **your forked version**, use the git section of this notebook with the side bar. Here is a [tutorial](https://blog.reviewnb.com/jupyterlab-git-extension/#git-workflow-clone).  \n",
    "If you want to submit code reviews, please see this [rule of participation](https://github.com/destination-earth/DestinE_ESA_GFTS/blob/main/docs/rule_of_participation.md)\n",
    "\n",
    "To be able to run this notebook you need to clone [pangeo-fish](https://github.com/IAOCEA/pangeo-fish) repository, you can reuse the previous steps you used before, no need to fork the repository this time. \n",
    "Once you have clone the pangeo-fish repository, you need to install it with the following command :  \n",
    "`!pip install pangeo-fish/`  /!\\ You might need to update the path to the pangeo-fish folder you cloned.  \n",
    "Normally, you can turn the first cell of the notebook into a code cell (select the cell and press Y key) and run it, wait for the installation to finish, then turn it back into a raw cell (select the cell and press R key), otherwise this cell will needlessly be runned by panel once you launch the dashboard. You can also use the notebook toolbar to change the cell type form code to raw.\n",
    "\n",
    "Normally, all the other library to use this notebook should already installed once this has been done. To start the preview with panel, click the blue pannel logo ![Panel logo](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAMAAABF0y+mAAAAM1BMVEUAcrUBd7jk6OuoxdsAcrUAbLP08vAAbbUAdLUAcrVQnMm6z99+rM86jcJ5sNElhL10qc1QLvfMAAAACnRSTlPP////////KCjO4FwxLQAAAJNJREFUKJGNktEShCAIRRXCCrX2/792aaYU3YXpTDM+nMQLGtawGIQtmE6s48S+kkRkSBEHMim6rMyYAA7u1EdSTj9kenYWRPGQsVNaWTnjxFzifGhvJbbcTp+V4yCj4gOA19rSImgkmvxAf2Ua5Vw267Ij5xTIbcUdgjc+f/DelenLXu5vTGs/EwNfuo963S23b1+vug28mwd6wAAAAABJRU5ErkJggg==) in the notebook tool bar.  \n",
    "This will open a new tab in your notebook that displays panel informations.\n",
    "\n",
    "Please note that all the tracks have been generated but note checked manually, some can display incoherent data. It will be checked and corrected later on.\n",
    "\n",
    "The trajectories available from S3 has been generated using this the papermill_launcher.ipynb notebook, see this notebook to understand more about it.\n",
    "Using this panel, user can examine the results of a computation. This helps to understand how the algorithm behave for different situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f8db5-1a4e-4ee6-9137-2fd43e4a2fd1",
   "metadata": {},
   "source": [
    "Here is a table that sums up all the generations\n",
    "You can use the value of generation name and set the variable generation_name to acces the corresponding generation.\n",
    "\n",
    "| generation_name | dataset                   | bbox                             | correction methods    | comment                                                                                                                                                                                                                                                 |\n",
    "|-----------------|---------------------------|----------------------------------|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| tracks          | IBI_MULTIYEAR_PHY_005_002 | latitude : 42,53- longitude -8,4 | None                  | 415 computed, 12 wrong tracks                                                                                                                                                                                                                                                    |\n",
    "| tracks_2        |    IBI_MULTIYEAR_PHY_005_002                       | latitude : 40,56- longitude -13,5                                  |Coastal variability, alpha=0.01 and MarkovAutoregression                      |431 computed, 32 wrong tracks. New bounding box that extends to all the fishes recapture positions.                                                                                                                                                                                                        |\n",
    "| tracks_3        |      IBI_MULTIYEAR_PHY_005_002                     |      latitude : 40,56- longitude -13,5                             |Coastal_variability, alpha=0.01 and MarkovAutoregression                     |395 computed, 39 wrong tracks. Implementation of a new method to correct issues due to anomalies in biologging data and test of a modelisation of the coastal variability This generation was not complete because there was an issue in the data.                                     |\n",
    "| tracks_4        |        IBI_MULTIYEAR_PHY_005_002                   |    latitude : 40,56- longitude -13,5                               | Coastal variability, alpha=0.01                      |421 computed, 60 wrong tracks. Same parameters as the previous one but with corrected input data. This is not implementing the anomalies in biologging fix.                                                                                                                            |\n",
    "| DK_1            |       IBI_MULTIYEAR_PHY_005_002                   |    latitude : 40,56- longitude -13,5                               | Markov autoregression | This generation is focusing only on fixing the issues observed from tags in region of dunkerque, which are subject to heat spikes anomalies. To correct this issue, a spike detection algorithm was implemented using a markovautoregression algorithm. |\n",
    "| DK_2            |      IBI_MULTIYEAR_PHY_005_002                     |    latitude : 40,56- longitude -13,5                               | Diff                  | This generation is using another techinque since markovautoregression was not detecting all the spikes.                                                                                                                                                 |\n",
    "| DK_3            |         IBI_MULTIYEAR_PHY_005_002                  |     latitude : 40,56- longitude -13,5                              | Diff                  | This generation is using a lower value of maximum speed, this contains only the results from the ones that has already failed before.                                                                                                                   |\n",
    "| DK_final        |    IBI_MULTIYEAR_PHY_005_002                       |    latitude : 40,56- longitude -13,5                               |Diff                       | In this folder, there is the data for all the DK tags. The corrected ones that from DK_3 and the ones that were already correct from the beginning.                                                                              |"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7112a4cf-be81-4c65-9c83-12f23c0d5751",
   "metadata": {},
   "source": [
    "# Install pangeo fish if necessary\n",
    "!pip install ~/git/pangeo-fish/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7d76e-9046-4305-abba-c97c04c10f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules.\n",
    "import os\n",
    "import xarray as xr\n",
    "from pint_xarray import unit_registry as ureg\n",
    "from pangeo_fish.io import open_tag\n",
    "import hvplot.xarray\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "import hvplot.pandas\n",
    "from pangeo_fish.tags import adapt_model_time, reshape_by_bins, to_time_slice\n",
    "import holoviews as hv\n",
    "import cmocean\n",
    "import panel as pn\n",
    "import numpy as np\n",
    "from pangeo_fish.io import read_trajectories\n",
    "from pangeo_fish import visualization\n",
    "import s3fs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06000a35-fc57-4244-b759-34307f76b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters to access data, no need to modify to load tags normally\n",
    "\n",
    "pn.extension()\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    client_kwargs={\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "    },\n",
    ")\n",
    "\n",
    "### Update this with the name of the folder where the results are stored\n",
    "generation_name = \"tracks_3\"\n",
    "\n",
    "# Tag list is the list of available tags\n",
    "remote_path = \"gfts-ifremer/tags/bargip\"\n",
    "tag_list_ = s3.ls(f\"{remote_path}/{generation_name}\")\n",
    "tag_list = [tag.replace(f\"{remote_path}/{generation_name}/\",\"\") for tag in tag_list_ if tag.replace(f\"{remote_path}/{generation_name}/\",\"\")]\n",
    "cloud_root = \"s3://gfts-ifremer/tags/bargip\"\n",
    "\n",
    "# tag_root specifies the root URL for tag data used for this computation.\n",
    "tag_root = f\"{cloud_root}/cleaned\"\n",
    "\n",
    "# scratch_root specifies the root directory where are GFTS computation data stored.\n",
    "scratch_root = f\"{cloud_root}/{generation_name}\"\n",
    "\n",
    "# storage_options specifies options for the filesystem storing and/or opening output files.\n",
    "storage_options = {\n",
    "    'anon': False, \n",
    "    # 'profile' : \"gfts\",\n",
    "    'client_kwargs': {\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "        \"region_name\": \"gra\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# bbox, bounding box, defines the latitude and longitude range for the analysis area.\n",
    "bbox = {\"latitude\": [40, 56], \"longitude\": [-13, 5]} \n",
    "# tramodes are the two types of track that have been computed for GFTS.\n",
    "track_modes = [\"mean\", \"mode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a37132-cb90-42b7-8306-1cc9aa850bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pn.cache stores all the plot outputs to avoid doing the computation every time. Might need to disable if manipulating a wide amount of files.\n",
    "# @pn.cache\n",
    "\n",
    "# Functions to plot the different visualization for a given tag id\n",
    "def plot_time_series(plot_type=\"time series\",tag_id=\"CB_A11071\"):\n",
    "    # load trajectories \n",
    "    trajectories = read_trajectories(track_modes,f\"{scratch_root}/{tag_id}\",storage_options, format=\"parquet\")\n",
    "\n",
    "    # Converting the trajectories to pandas DataFrames to access data easily\n",
    "    mean_df = trajectories.trajectories[0].df\n",
    "    mode_df = trajectories.trajectories[1].df\n",
    "\n",
    "    tag = open_tag(tag_root, tag_id)\n",
    "    # time_slice = to_time_slice(tag[\"tagging_events/time\"])\n",
    "\n",
    "    time_slice = to_time_slice(tag[\"tagging_events/time\"])\n",
    "    \n",
    "    time = tag[\"dst\"].ds.time\n",
    "    cond = (time <= time_slice.stop) & (time >= time_slice.start)\n",
    "    \n",
    "    tag_log = tag[\"dst\"].ds.where(cond,drop=True)\n",
    "    \n",
    "    min_ = tag_log.time[0]\n",
    "    max_ = tag_log.time[-1]\n",
    "    \n",
    "    time_slice = slice(min_.data, max_.data)\n",
    "\n",
    "    \n",
    "    tag_log = tag[\"dst\"].ds.sel(time=time_slice)        \n",
    "    \n",
    "    # Creating pandas series for xarrray dataset\n",
    "    mean_lon_ = pd.Series(mean_df.geometry.x,name=\"longitude\")\n",
    "    mean_lat_ = pd.Series(mean_df.geometry.y,name=\"latitude\")\n",
    "    mode_lon_ = pd.Series(mode_df.geometry.x,name=\"longitude\")\n",
    "    mode_lat_ = pd.Series(mode_df.geometry.y,name=\"latitude\")\n",
    "    \n",
    "    # Creating xarray datasets\n",
    "    mean_coords = xr.Dataset(pd.concat([mean_lon_, mean_lat_], axis=1))\n",
    "    mode_coords = xr.Dataset(pd.concat([mode_lon_, mode_lat_], axis=1))\n",
    "    \n",
    "    # Assigning dataarrays to variables\n",
    "    mean_lon = mean_coords[\"longitude\"]\n",
    "    mean_lat = mean_coords[\"latitude\"]\n",
    "    mode_lon = mode_coords[\"longitude\"]\n",
    "    mode_lat = mode_coords[\"latitude\"]\n",
    "\n",
    "    tag_log[\"depth\"] = tag_log[\"pressure\"]\n",
    "    temp_plot = tag_log[\"temperature\"].hvplot(color=\"Red\",title=\"Temperature (Â°C)\",grid=True,height=200,width=600)\n",
    "    depth_plot = (-tag_log[\"depth\"]).hvplot(color =\"Blue\",title=\"Depth (m)\",grid=True,height=200,width=600)\n",
    "    lon_plot = (mean_lat.hvplot(label=\"mean\",clim=[mean_lat_.min(),mean_lat_.max()]) * mode_lat.hvplot(label = \"mode\",clim=[mode_lat_.min(),mean_lat_.max()])).opts(height=200,width=600,show_grid=True,title = \"Fish latitude over time\")\n",
    "    lat_plot = (mean_lon.hvplot(label=\"mean\",clim=[mean_lon_.min(),mean_lat_.max()]) * mode_lon.hvplot(label = \"mode\",clim=[mode_lon_.min(),mean_lat_.max()])).opts(height=200,width=600,show_grid=True,title = \"Fish longitude over time\")\n",
    "\n",
    "    return (temp_plot + depth_plot + lon_plot + lat_plot).cols(1)\n",
    "\n",
    "\n",
    "def plot_anomaly(tag_id=\"CB_A11071\"):\n",
    "    tag = open_tag(tag_root, tag_id)\n",
    "    time_slice = to_time_slice(tag[\"tagging_events/time\"])\n",
    "    tag_log = tag[\"dst\"].ds.sel(time=time_slice) \n",
    "    ds = tag_log\n",
    "    # Calculate the differences\n",
    "    ds['temp_diff'] = ds.temperature.diff('time')\n",
    "    \n",
    "    # Define a threshold for significant temperature rise\n",
    "    threshold = 0.2\n",
    "    \n",
    "    # Identify significant rises\n",
    "    ds['event'] = ds['temp_diff'] > threshold\n",
    "    \n",
    "    # Extract significant points\n",
    "    significant_points = ds.where(ds['event'], drop=True)\n",
    "    \n",
    "    # Plot the time series\n",
    "    time_series_plot = ds.temperature.hvplot(line_color='blue', label='Temperature',width=1000,height=500)\n",
    "    \n",
    "    # Plot the significant rise points\n",
    "    significant_points_plot = significant_points.temperature.hvplot.scatter(\n",
    "        color='red', marker='x', size=100, label='Significant Rise',width=1000,height=500\n",
    "    )\n",
    "        \n",
    "    # Display the combined plot\n",
    "    return hv.Overlay([time_series_plot, significant_points_plot]).opts(width=1000,height=500)\n",
    "\n",
    "def plot_track(tag_id=\"CB_A11071\"):\n",
    "    sigma = pd.read_json(f\"{scratch_root}/{tag_id}/parameters.json\").to_dict()[0][\"sigma\"]\n",
    "    trajectories = read_trajectories(track_modes,f\"{scratch_root}/{tag_id}\",storage_options, format=\"parquet\")\n",
    "\n",
    "    # Converting the trajectories to pandas DataFrames to access data easily\n",
    "    mean_df = trajectories.trajectories[0].df\n",
    "    mode_df = trajectories.trajectories[1].df\n",
    "    \n",
    "    # Adding month data\n",
    "    mean_df[\"month\"] = mean_df.index.month\n",
    "    mode_df[\"month\"] = mode_df.index.month\n",
    "    \n",
    "    # Converting back to trajectories\n",
    "    mean_traj = mpd.Trajectory(mean_df,traj_id=mean_df.traj_id.drop_duplicates().values[0])\n",
    "    mode_traj = mpd.Trajectory(mode_df,traj_id=mode_df.traj_id.drop_duplicates().values[0])\n",
    "    trajectories = mpd.TrajectoryCollection([mean_traj,mode_traj])\n",
    "    \n",
    "    traj_plots = [\n",
    "        traj.hvplot(c=\"month\",tiles=\"CartoLight\",cmap=\"rainbow\", title=f\"{tag_id} , {traj.id}, {sigma}\",width=375,height=375)\n",
    "        for traj in trajectories.trajectories\n",
    "    ]\n",
    "    \n",
    "    return hv.Layout(traj_plots).cols(1)\n",
    "    \n",
    "def plot_emission(tag_id=\"CB_A11071\"):\n",
    "    ## Might not work if dask involved or slider involved, I have to test\n",
    "    emission = (\n",
    "        xr.open_dataset(\n",
    "            f\"{scratch_root}/{tag_id}/combined.zarr\",\n",
    "            engine=\"zarr\",\n",
    "            chunks={},\n",
    "            inline_array=True,\n",
    "            storage_options=storage_options,\n",
    "        )\n",
    "        .rename_vars({\"pdf\": \"emission\"})\n",
    "    )\n",
    "    \n",
    "    states = (\n",
    "        xr.open_dataset(\n",
    "            f\"{scratch_root}/{tag_id}/states.zarr\", \n",
    "            engine=\"zarr\", \n",
    "            chunks={}, \n",
    "            inline_array=True,\n",
    "            storage_options=storage_options,\n",
    "        ).where(emission[\"mask\"])\n",
    "    )\n",
    "    \n",
    "    data = xr.merge([states, emission.drop_vars([\"mask\"])])\n",
    "    plot1 = visualization.plot_map(data[\"states\"].sel(time=slice(\"2015-09-04\",\"2015-09-10\")),bbox,cmap=\"cool\").opts(height=350,width=600)\n",
    "    plot2 = visualization.plot_map(data[\"emission\"].sel(time=slice(\"2015-09-04\",\"2015-09-10\")),bbox,cmap=\"cool\").opts(height=350,width=600)\n",
    "    plot=hv.Layout([plot1, plot2]).cols(1)        \n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da5b0f-ae24-4e3e-9502-46e9c4c37d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel parameters\n",
    "value=tag_list[0]\n",
    "#Initalizing the widget for tag selection \n",
    "tag_widget = pn.widgets.Select(name=\"tag_id\", value=value, options=tag_list)\n",
    "\n",
    "#Binding widget with the plots\n",
    "\n",
    "time_plot = pn.bind(plot_time_series,tag_id=tag_widget)\n",
    "track_plot = pn.bind(plot_track,tag_id=tag_widget)\n",
    "# Commenting emission because it's too long to load panel \n",
    "# emission_plot = pn.bind(plot_emission,tag_id=tag_widget)\n",
    "track_emission = pn.Row(time_plot,track_plot)\n",
    "\n",
    "#Combining plots with the widget\n",
    "plots = pn.Row(tag_widget,track_emission)\n",
    "\n",
    "pn.template.FastListTemplate(\n",
    "    site=\"Tag data display\",\n",
    "    title=\"plots\",\n",
    "    main=plots,\n",
    ").servable();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff000e3-47dd-41fd-9cbb-cd22a5193ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd1a79-6125-4f6e-882c-2be556d68128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

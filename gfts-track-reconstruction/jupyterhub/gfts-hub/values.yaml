coreNodeSelector: &coreNodeSelector
  hub.jupyter.org/node-purpose: core

jupyterhub:
  proxy:
    chp:
      nodeSelector: *coreNodeSelector
    service:
      type: ClusterIP
  ingress:
    enabled: true
    annotations:
      ingress.kubernetes.io/proxy-body-size: 64m
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
    hosts:
      - gfts.minrk.net
    tls:
      - secretName: tls-jupyterhub
        hosts:
          - gfts.minrk.net

  singleuser:
    events: true
    startTimeout: 600
    storage:
      capacity: 100Gi
      homeMountPath: /home/jovyan
    image:
      name: "c63eqfuv.c1.gra9.container-registry.ovh.net/gfts/jupyterhub-user"
      tag: "set-by-chartpress"
      pullPolicy: Always
    networkPolicy:
      egress:
        # allow access to kbatch-proxy
        - to:
            - podSelector:
                matchLabels:
                  app.kubernetes.io/name: kbatch-proxy
          ports:
            - protocol: TCP
              port: 80
    memory:
      limit: 24G
      guarantee: 8G
    cpu:
      limit: 6
      guarantee: 2
    extraEnv:
      CULL_CONNECTED: "1"
      CULL_TIMEOUT: "1800"
      CULL_KERNEL_TIMEOUT: "1800"
      CULL_INTERVAL: "120"
      GH_SCOPED_CREDS_CLIENT_ID: Iv1.f4a7db20c671f599
      GH_SCOPED_CREDS_APP_URL: https://github.com/apps/gfts-jupyterhub
      KBATCH_URL: http://kbatch-proxy
      KBATCH_S3_CODE_DIR: s3://gfts-ifremer/kbatch/{username}
      AWS_ENDPOINT_URL_S3: "https://s3.gra.perf.cloud.ovh.net"
      AWS_DEFAULT_REGION: gra
      AWS_REQUEST_CHECKSUM_CALCULATION: WHEN_REQUIRED

      # rclone config (https://rclone.org/docs/#environment-variables)
      RCLONE_CONFIG_GFTS_TYPE: s3
      RCLONE_CONFIG_GFTS_PROVIDER: Other
      RCLONE_CONFIG_GFTS_ENV_AUTH: "true"
      RCLONE_CONFIG_GFTS_REGION: gra
      RCLONE_CONFIG_GFTS_endpoint: https://s3.gra.perf.cloud.ovh.net

      RCLONE_CONFIG_CMARINE_TYPE: s3
      RCLONE_CONFIG_CMARINE_PROVIDER: Other
      RCLONE_CONFIG_CMARINE_endpoint: https://s3.waw3-1.cloudferro.com
      RCLONE_CONFIG_CMARINE_ACL: public-read

    profileList:
      - display_name: "GFTS environment"
        slug: default
        description: Select resource allocation and image
        default: true
        profile_options:
          image:
            display_name: Image
            choices:
              default:
                default: true
                display_name: "Default GFTS environment"
                kubespawner_override: {}
              pangeo:
                display_name: "Stock Pangeo environment"
                kubespawner_override:
                  image: quay.io/pangeo/pangeo-notebook:2025.01.24
              lastknowngood:
                display_name: "Last known good image (before s3 put problems)"
                kubespawner_override:
                  image: >-
                    c63eqfuv.c1.gra9.container-registry.ovh.net/gfts/jupyterhub-user:0.0.1-0.dev.git.336.h58d2d48
          resource_allocation:
            display_name: Resource allocation
            choices:
              default:
                default: true
                display_name: Default (24 GB / 2-6 CPU)
                kubespawner_override:
                  mem_guarantee: 8G
                  mem_limit: 24G
                  cpu_guarantee: 2
                  cpu_limit: 6
              big60:
                display_name: 60 GB / 8 CPU
                kubespawner_override:
                  mem_guarantee: 60e9
                  mem_limit: 60e9
                  cpu_guarantee: 7
                  cpu_limit: 8
              big120:
                display_name: 120 GB / 15 CPU
                kubespawner_override:
                  mem_guarantee: 120e9
                  mem_limit: 120e9
                  cpu_guarantee: 15
                  cpu_limit: 15
              big160:
                display_name: 160 GB / 20 CPU
                kubespawner_override:
                  mem_guarantee: 160e9
                  mem_limit: 160e9
                  cpu_guarantee: 20
                  cpu_limit: 20
              big240:
                display_name: 240 GB / 30 CPU
                kubespawner_override:
                  mem_guarantee: 240e9
                  mem_limit: 240e9
                  cpu_guarantee: 30
                  cpu_limit: 30
              huge:
                display_name: Huge (480 GB / 60 CPU)
                kubespawner_override:
                  mem_guarantee: 480e9
                  mem_limit: 480e9
                  cpu_guarantee: 60
                  cpu_limit: 60
                  node_selector:
                    gfts.destination-earth.eu/size: big512

    extraFiles:
      allow-iframe:
        mountPath: /etc/jupyter/jupyter_server_config.json
        data:
          ServerApp:
            tornado_settings:
              headers:
                Content-Security-Policy: frame-ancestors 'self'
    initContainers:
      # seems to be something in the volume mount resetting permissions
      # .ssh doesn't like to be public, so ensure it's private
      - name: ssh-permissions-and-rclone-config
        image: busybox:1.36
        command:
          - sh
          - "-c"
          - |
            if [ -d /homedir/.ssh ]; then
              chmod -R og= /homedir/.ssh
            fi
            # fix permissions caused by earlier creation of rclone config file
            if [ -d /homedir/.config ]; then
              chown -R 1000:1000 /homedir/.config
            fi
        volumeMounts:
          - mountPath: /homedir
            name: volume-{username}

  prePuller:
    hook:
      enabled: false
    continuous:
      enabled: true

  hub:
    nodeSelector: *coreNodeSelector
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/path: /hub/metrics
    config:
      JupyterHub:
        authenticate_prometheus: false
      Authenticator:
        allowed_users:
          - minrk
          - tinaok
          - yellowcap
          - danielfdsilva
          - annefou
          - aderrien7
          - jmdelouis
          - mwoillez
          - marinerandon
          - keewis
          - quentinmaz
          - davidcasalsvliz
          - capetienne
          - corentin-hue
      KubeSpawner:
        pod_security_context:
          fsGroup: 100
          fsGroupChangePolicy: OnRootMismatch

    extraConfig:
      load_creds: |
        import json
        from pathlib import Path
        creds_path = Path("/srv/s3creds.json")

        def load_creds(spawner):
            with creds_path.open() as f:
                all_creds = json.load(f)
            name = spawner.user.name
            if name in all_creds:
                spawner.log.info(f"Loading AWS credentials for {name}")
                creds = all_creds[name]
            else:
                spawner.log.info(f"Using default AWS credentials for {name}")
                creds = all_creds["_default"]
            spawner.environment["AWS_ACCESS_KEY_ID"] = creds["aws_access_key_id"]
            spawner.environment["AWS_SECRET_ACCESS_KEY"] = creds["aws_secret_access_key"]
          
        c.KubeSpawner.pre_spawn_hook = load_creds
    loadRoles:
      server:
        scopes:
          - access:servers!server
          - users:activity!user
          - access:services!service=kbatch
      management:
        scopes:
          - admin-ui
          - access:servers
          - list:users
          - read:users
          - servers
          - groups
        users:
          - annefou
          - minrk
      kbatch-users:
        scopes:
          - "access:services!service=kbatch"
        users:
          - minrk
          - annefou
          - tinaok
          - quentinmaz
          - capetienne
    services:
      kbatch-proxy: {}

  scheduling:
    userScheduler:
      enabled: true
    podPriority:
      enabled: true
    userPlaceholder:
      enabled: true
      replicas: 0
    userPods:
      nodeAffinity:
        # 'prefer' lets users run on small core nodes
        # to save cost
        # 'require' protects core nodes from user resource consumption
        matchNodePurpose: require
    corePods:
      nodeAffinity:
        matchNodePurpose: require

grafana:
  nodeSelector: *coreNodeSelector
  grafana.ini:
    auth.anonymous:
      enabled: true
      org_name: Main Org.
      org_role: Viewer
    auth.basic:
      enabled: true
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
    hosts:
      - grafana.gfts.minrk.net
    tls:
      - hosts:
          - grafana.gfts.minrk.net
        secretName: tls-grafana
  persistence:
    size: 2Gi
    enabled: true
    accessModes:
      - ReadWriteOnce
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: prometheus
          orgId: 1
          type: prometheus
          url: https://prometheus.gfts.minrk.net
          isDefault: true
          editable: false

prometheus:
  nodeSelector: *coreNodeSelector
  nodeExporter:
    updateStrategy:
      type: RollingUpdate
  alertmanager:
    enabled: false
  pushgateway:
    enabled: false
  rbac:
    create: true
  server:
    nodeSelector: *coreNodeSelector
    persistentVolume:
      size: 64Gi
    podLabels:
      hub.jupyter.org/network-access-hub: "true"
    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: nginx
        kubernetes.io/tls-acme: "true"
      hosts:
        - prometheus.gfts.minrk.net
      tls:
        - hosts:
            - prometheus.gfts.minrk.net
          secretName: tls-prometheus
    retention: 356d
  kube-state-metrics:
    metricLabelsAllowlist:
      - pods=[app,component]
      - nodes=[*]

ingress-nginx:
  rbac:
    create: true
  statsExporter:
    service:
      annotations:
        prometheus.io/scrape: "true"
  controller:
    nodeSelector: *coreNodeSelector
    replicaCount: 2
    scope:
      enabled: true
    config:
      # Allow POSTs of up to 64MB, for large notebook support.
      proxy-body-size: 64m
    stats:
      enabled: true
    service:
      # Preserve client IPs
      externalTrafficPolicy: Local

dask-gateway:
  gateway:
    extraConfig:
      resource-limits: |
        # for now, limit resources to very small clusters
        c.ClusterConfig.cluster_max_cores = 8
        c.ClusterConfig.cluster_max_memory = "8 G"

        # shutdown idle clusters after one hour
        c.ClusterConfig.idle_timeout = 3600

kbatch-proxy:
  fullnameOverride: kbatch-proxy
  image:
    repository: ghcr.io/minrk/kbatch-proxy
    tag: "gfts"
    pullPolicy: Always
  app:
    # jupyterhub_service_prefix: /services/kbatch-proxy
    # cannot use internal ip
    # pending https://github.com/kbatch-dev/helm-chart/pull/6
    jupyterhub_api_url: https://gfts.minrk.net/hub/api/
    extra_env:
      JUPYTERHUB_SERVICE_NAME: kbatch
      KBATCH_JOB_TEMPLATE_FILE: /srv/job_template.yaml
      KBATCH_PROFILE_FILE: /srv/profiles.yaml
      # the time before a job is deleted after finishing
      # ideally, we use a mutating webhook
      # to keep failures for longer than success
      # default: 1 week
      KBATCH_JOB_TTL_SECONDS_AFTER_FINISHED: 604800
    extraFiles:
      job_template:
        mountPath: /srv/job_template.yaml
        data:
          apiVersion: batch/v1
          kind: Job
          metadata:
            labels:
              app.kubernetes.io/managed-by: kbatch
          spec:
            template:
              metadata:
                labels:
                  app.kubernetes.io/managed-by: kbatch
              spec:
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: hub.jupyter.org/node-purpose
                              operator: In
                              values:
                                - user
      profiles:
        mountPath: /srv/profiles.yaml
        data:
          default:
            resources:
              requests:
                cpu: "2"
                memory: "8G"
              limits:
                cpu: "2"
                memory: "8G"
          big60:
            resources:
              requests:
                cpu: "7"
                memory: "60G"
              limits:
                cpu: "8"
                memory: "60G"
          big120:
            resources:
              requests:
                cpu: "14"
                memory: "120G"
              limits:
                cpu: "15"
                memory: "120G"
          big160:
            resources:
              requests:
                cpu: "20"
                memory: "160G"
              limits:
                cpu: "20"
                memory: "160G"
          big240:
            resources:
              requests:
                cpu: "29"
                memory: "240G"
              limits:
                cpu: "30"
                memory: "240G"
          huge:
            resources:
              requests:
                cpu: "59"
                memory: "480G"
              limits:
                cpu: "60"
                memory: "480G"
            node_affinity_required:
              - matchExpressions:
                  - key: gfts.destination-earth.eu/size
                    operator: In
                    values:
                      - big512

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c0784b-1358-4926-827b-9808d153d39b",
   "metadata": {},
   "source": [
    "# Copernicus data exploration\n",
    "___\n",
    "#### In this notebook, we search for the different caracteristics of the required data from copernicus marine sevices in order to add them to a yaml file that will store all the uris for relevant products and help us improve how we access data from cmems.\n",
    "\n",
    "#### Here is a little list of definitions of the different keywords use in this notebook :\n",
    "___\n",
    "- **Product** : A product is a set of dataset avaible from the [Copenicus Marine Data Store](https://data.marine.copernicus.eu/products). It is distinguishable thanks to is product ID, example : **GLOBAL_ANALYSISFORECAST_PHY_001_024**.\n",
    "- **Dataset** : A dataset is a collection of data points, each described by multiple variables. For a given dataset, the coordinates are fixed and indicate where data is present for each variable for a given coordinate. In cmems a dataset has a unique identifier such as : **cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m** and contains some info inside, such as the spatial and temporal resolution. Here, 0.083deg means that the model has a 0.083*0.083 spatial resoltion and the 1D means that it's a daily temporal resolution. A dataset is accessible via a URI.\n",
    "- **Variable** : A variable is a part of a dataset, it depends of one or more coordinates. For GFTS, we are interested in 3 particular variables :\n",
    "> - **thetao :** sea_water_potential_temperature. Allows us to calculate the difference between the actual temperature and the measured temperature from biologging. Depends on Latitude, Longitude, Depth and time.\n",
    "> - **zos :** sea_surface_height_above_geoid. Sea surface height relativly to the geoid. Depends on Latitude, Longitude and time.\n",
    "> - **deptho :** sea_floor_depth_below_geoid. Distance between the geoid and the sea floor. Depends on Latitude and Longitude\n",
    "- **Coordinate :** Describe a Variable, where and when the data exists.\n",
    "___ \n",
    "First you need to get to [Copernicus Marine Data Store](https://data.marine.copernicus.eu/products), choose a product that can match your usage and then copy the product ID in the classification section on the product page."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf4939db-4e02-4661-bc2d-370c6adedeaa",
   "metadata": {},
   "source": [
    "# Run this cell if copernicusmarine library not installed on your system\n",
    "!pip install copernicusmarine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10401471-d7e2-40e7-b9bd-69438ee4245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copernicusmarine\n",
    "import yaml\n",
    "import intake\n",
    "import xarray as xr\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39d0e6-b394-457a-a665-e5bda39251a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    client_kwargs={\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93c207-d728-4e62-aad9-e3bad957a306",
   "metadata": {},
   "source": [
    "## **1. Defining the functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffaf4d2-efe8-4249-8cb1-286bbbd68698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_copernicus_dict(product_id=\"GLOBAL_ANALYSISFORECAST_PHY_001_024\"):\n",
    "    \"\"\"\n",
    "    Generates a dictionary containing metadata information about the specified\n",
    "    product from the Copernicus Marine service. This dictionary can be used for\n",
    "    saving to a YAML file.\n",
    "\n",
    "    Args:\n",
    "        product_id (str): The ID of the product to retrieve information about.\n",
    "                          Default is \"GLOBAL_ANALYSISFORECAST_PHY_001_024\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing metadata and variable information for the product.\n",
    "    \"\"\"\n",
    "    products_info = {}\n",
    "\n",
    "    # Open the CMEMS catalog and retrieve product details\n",
    "    catalogue = copernicusmarine.describe(\n",
    "        include_datasets=True,\n",
    "        contains=[product_id],\n",
    "    )\n",
    "\n",
    "    # Iterate through each dataset in the product\n",
    "    for dataset in tqdm(catalogue[\"products\"][0][\"datasets\"], desc=\"Gathering data\"):\n",
    "        dataset_id = dataset[\"dataset_id\"]\n",
    "\n",
    "        for service in dataset[\"versions\"][0][\"parts\"][0][\"services\"]:\n",
    "            # Check if the service type is not 'original-files' and contains '.zarr'\n",
    "            if (service[\"service_type\"][\"service_name\"] != \"original-files\") & (\n",
    "                \".zarr\" in service[\"uri\"]\n",
    "            ):\n",
    "                # Collect the URI of the service\n",
    "                uri = service[\"uri\"]\n",
    "                variable_info = {}\n",
    "                variable_metadata = {}\n",
    "                data = xr.open_zarr(uri)\n",
    "                data_vars = data.data_vars\n",
    "\n",
    "                # Filters for reaching only the variables we need for GFTS\n",
    "                if \"thetao\" in data_vars or \"zos\" in data_vars or \"deptho\" in data_vars:\n",
    "                    # Collect data for each variable in the dataset\n",
    "                    for var in data_vars:\n",
    "                        standard_name = data[var].attrs[\"standard_name\"]\n",
    "                        units = data[var].attrs[\"units\"]\n",
    "\n",
    "                        coords_info = {}\n",
    "                        # Collect coordinates information for the variable\n",
    "                        for coord in data[var].coords:\n",
    "                            da = data[var][coord]\n",
    "                            if da.attrs != {} and \"units\":\n",
    "                                coords_info[coord] = {\n",
    "                                    \"units\": data[var][coord].attrs[\"units\"],\n",
    "                                }\n",
    "\n",
    "                            if \"step\" in da.attrs:\n",
    "                                variable_metadata[\n",
    "                                    \"spatial_resolution (degrees)\"\n",
    "                                ] = data[var][coord].attrs[\"step\"]\n",
    "\n",
    "                            coords_info[coord] = {\n",
    "                                \"min_val\": float(da[coord].min().data),\n",
    "                                \"max_val\": float(da[coord].max().data),\n",
    "                            }\n",
    "\n",
    "                        if \"latitude\" in coords_info and \"longitude\" in coords_info:\n",
    "                            bbox = {\n",
    "                                \"latitude\": [\n",
    "                                    coords_info[\"latitude\"][\"min_val\"],\n",
    "                                    coords_info[\"latitude\"][\"max_val\"],\n",
    "                                ],\n",
    "                                \"longitude\": [\n",
    "                                    coords_info[\"longitude\"][\"max_val\"],\n",
    "                                    coords_info[\"longitude\"][\"min_val\"],\n",
    "                                ],\n",
    "                            }\n",
    "\n",
    "                        if \"time\" in data[var].coords:\n",
    "                            step = np.unique(data[var][\"time\"].diff(dim=\"time\")).astype(\n",
    "                                \"timedelta64[h]\"\n",
    "                            )\n",
    "                            units = str(data[var][\"time\"].dtype)\n",
    "                            coords_info[\"time\"] = {\n",
    "                                \"min_val\": str(data[var][\"time\"].min().data),\n",
    "                                \"max_val\": str(data[var][\"time\"].max().data),\n",
    "                                \"step\": int(step.astype(int)[0]),\n",
    "                                \"step_unit\": str(step.dtype),\n",
    "                                \"units\": units,\n",
    "                            }\n",
    "\n",
    "                        try:\n",
    "                            bbox\n",
    "                        except NameError:\n",
    "                            bbox = None\n",
    "                        # Create metadata for a given variable\n",
    "                        variable_metadata[var] = {\n",
    "                            \"standard_name\": standard_name,\n",
    "                            \"coordinates\": coords_info,\n",
    "                            \"dataset_id\": dataset_id,\n",
    "                            \"bbox\": bbox,\n",
    "                            \"units\": units,\n",
    "                        }\n",
    "\n",
    "                    # Add metadata to the variable information\n",
    "                    variable_info[\"metadata\"] = {\n",
    "                        \"description\": \"Variables available in this product\",\n",
    "                        \"variable\": variable_metadata,\n",
    "                    }\n",
    "\n",
    "                    # Specify the driver information\n",
    "                    variable_info[\"driver\"] = \"zarr\"\n",
    "\n",
    "                    # Modify the URI with the parameter aproach for Intake catlaog\n",
    "                    if \"timeChunked\" in uri:\n",
    "                        variable_info[\"default\"] = \"time\"\n",
    "                        variable_info[\"allowed\"] = [\"time\", \"geo\"]\n",
    "                        uri = uri.replace(\"time\", \"{{ chunk }}\")\n",
    "\n",
    "                    elif \"geoChunked\" in uri:\n",
    "                        variable_info[\"default\"] = \"time\"\n",
    "                        variable_info[\"allowed\"] = [\"time\", \"geo\"]\n",
    "                        uri = uri.replace(\"geo\", \"{{ chunk }}\")\n",
    "\n",
    "                    variable_info[\"args\"] = {\"urlpath\": uri, \"consolidated\": True}\n",
    "\n",
    "                    # Add the variable information to the product info dictionary\n",
    "                    products_info[dataset_id] = variable_info\n",
    "\n",
    "    return products_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8375e4-f313-4ad2-8b88-22b440171032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml(file):\n",
    "    \"\"\"\n",
    "    Reads a YAML file and returns its content.\n",
    "\n",
    "    Parameters:\n",
    "    file (str): Path to the YAML file.\n",
    "\n",
    "    Returns:\n",
    "    dict: Parsed content of the YAML file.\n",
    "    None: If there's an error during parsing.\n",
    "    \"\"\"\n",
    "    with open(file, \"r\") as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            return None\n",
    "\n",
    "\n",
    "def update_copernicus_sources(file, d):\n",
    "    \"\"\"\n",
    "    Updates the 'sources' section of a YAML file with the provided dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    file (str): Path to the YAML file to be updated.\n",
    "    d (dict): Dictionary containing new source data to be added.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated content of the YAML file.\n",
    "    \"\"\"\n",
    "    copernicus_file = read_yaml(file)\n",
    "\n",
    "    # Check if 'sources' is None and initialize if needed\n",
    "    if copernicus_file[\"sources\"] is {}:\n",
    "        copernicus_file[\"sources\"] = d\n",
    "    else:\n",
    "        # Update existing 'sources' with new data\n",
    "        for key in d.keys():\n",
    "            copernicus_file[\"sources\"][key] = d[key]\n",
    "\n",
    "    return copernicus_file\n",
    "\n",
    "\n",
    "def generate_template(filename):\n",
    "    \"\"\"\n",
    "    Generates a template YAML file with predefined structure and saves it.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path where the template YAML file will be saved.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(filename):\n",
    "        # Create the file if it does not exist\n",
    "        open(filename, \"a\").close()\n",
    "\n",
    "    template = {\n",
    "        \"description\": \"Uris and data about the relevant products for GFTS project\",\n",
    "        \"metadata\": {\"version\": 1},\n",
    "        \"sources\": {},\n",
    "    }\n",
    "    write_yaml(filename, template)\n",
    "\n",
    "\n",
    "def generate_master_template(filename):\n",
    "    \"\"\"\n",
    "    Generates a template YAML file with predefined structure and saves it.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path where the template YAML file will be saved.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(filename):\n",
    "        # Create the file if it does not exist\n",
    "        open(filename, \"a\").close()\n",
    "\n",
    "    template = {\n",
    "        \"description\": \"Master catalog to read data from copernicus marine data store\",\n",
    "        \"metadata\": {\"version\": 1},\n",
    "        \"sources\": {},\n",
    "    }\n",
    "    write_yaml(filename, template)\n",
    "\n",
    "\n",
    "def write_yaml(file, content):\n",
    "    \"\"\"\n",
    "    Writes content to a YAML file.\n",
    "\n",
    "    Parameters:\n",
    "    file (str): Path to the YAML file.\n",
    "    content (dict): Content to be written to the YAML file.\n",
    "    \"\"\"\n",
    "    with open(file, \"w\") as stream:\n",
    "        try:\n",
    "            yaml.dump(content, stream, default_flow_style=False, allow_unicode=True)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc41537-95fb-425b-8071-9c1ee589e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_product_yml(product_id, storage_location=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a product YAML file for a specified product ID and stores it in the given location.\n",
    "\n",
    "    This function creates a YAML file for a product by generating a template, updating it with\n",
    "    product-specific data, and then writing the updated catalog to the file. The generated YAML\n",
    "    file will be referenced in the master catalog.\n",
    "\n",
    "    Parameters:\n",
    "    product_id (str): The unique identifier for the product.\n",
    "    storage_location (str): The directory path where the YAML file will be stored. Default is an empty string.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the full path for the product YAML file\n",
    "    full_path = f\"{storage_location}{product_id}.yml\"\n",
    "\n",
    "    # Generate a template for the YAML file\n",
    "    generate_template(full_path)\n",
    "\n",
    "    # Generate a dictionary with product-specific data\n",
    "    data = generate_copernicus_dict(product_id)\n",
    "\n",
    "    # Update the product catalog with the generated data\n",
    "    update_cat = update_copernicus_sources(full_path, data)\n",
    "\n",
    "    # Write the updated catalog to the YAML file\n",
    "    write_yaml(full_path, update_cat)\n",
    "\n",
    "    # Print a message indicating successful creation of the YAML file\n",
    "    print(f\"{product_id} yml file created at {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2f39b-06c7-4869-96a1-5b72e808f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_master_cat(master_cat_path, copernicus_cat_path):\n",
    "    \"\"\"\n",
    "    Updates the master catalog with references to new product catalogs found in the specified directory.\n",
    "\n",
    "    This function checks if the master catalog file exists. If not, it generates a master template.\n",
    "    Then, it reads the master catalog, updates it with references to any new product catalogs\n",
    "    found in the specified directory, and writes the updated master catalog back to the file.\n",
    "\n",
    "    Warning : The master catalog created with this function will only create a catalog with a local access path.\n",
    "    Thus the catalog created is not meant to be pushed to the s3 bucket.\n",
    "    The catalog that will be pushed to the bucket should be created with the function update_master_cat_remote()\n",
    "\n",
    "    Parameters:\n",
    "    master_cat_path (str): The file path to the master catalog YAML file.\n",
    "    copernicus_cat_path (str): The directory path containing the Copernicus product catalog YAML files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the master catalog file exists\n",
    "    if not os.path.exists(master_cat_path):\n",
    "        generate_master_template(\n",
    "            master_cat_path\n",
    "        )  # Generate master template if it does not exist\n",
    "\n",
    "    # Read the master catalog YAML file\n",
    "    master = read_yaml(master_cat_path)\n",
    "\n",
    "    # Iterate through all files in the Copernicus catalog directory\n",
    "    for file in os.listdir(copernicus_cat_path):\n",
    "        if \".yml\" in file:  # Process only YAML files\n",
    "            product_dict = {\n",
    "                \"args\": {\n",
    "                    \"path\": f\"{copernicus_cat_path}{file}\"  # Path to the product catalog file\n",
    "                },\n",
    "                \"description\": \"Uris and data about a relevant product for GFTS project\",\n",
    "                \"driver\": \"intake.catalog.local.YAMLFileCatalog\",\n",
    "                \"metadata\": {\"version\": 1},\n",
    "            }\n",
    "            product_id = file.replace(\".yml\", \"\")\n",
    "            master[\"sources\"][product_id] = product_dict\n",
    "\n",
    "    # Write the updated master catalog back to the file\n",
    "    write_yaml(master_cat_path, master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba38c5-8426-4689-a7d9-a8bd1b180581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_master_cat_remote(master_cat_path, s3):\n",
    "    \"\"\"\n",
    "    Updates the master catalog for the s3 bucket with references to new product catalogs found in the specified directory.\n",
    "\n",
    "    This function checks if the master catalog file exists. If not, it generates a master template.\n",
    "    Then, it reads the master catalog, updates it with references to any new product catalogs\n",
    "    found in the specified directory, and writes the updated master catalog back to the file.\n",
    "\n",
    "    Parameters:\n",
    "    master_cat_path (str): The file path to the master catalog YAML file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the master catalog file exists\n",
    "    if not os.path.exists(master_cat_path):\n",
    "        generate_master_template(\n",
    "            master_cat_path\n",
    "        )  # Generate master template if it does not exist\n",
    "\n",
    "    # Read the master catalog YAML file\n",
    "    master = read_yaml(master_cat_path)\n",
    "\n",
    "    # Iterate through all files in the Copernicus catalog directory\n",
    "    s3_path = \"gfts-ifremer/copernicus_catalogs/product_catalogs/\"\n",
    "    for file in s3.ls(s3_path):\n",
    "        if \".yml\" in file:  # Process only YAML files\n",
    "            product_dict = {\n",
    "                \"args\": {\n",
    "                    \"path\": f\"s3://{file}\"  # Path to the product catalog file\n",
    "                },\n",
    "                \"description\": \"Uris and data about a relevant product for GFTS project\",\n",
    "                \"driver\": \"intake.catalog.local.YAMLFileCatalog\",\n",
    "                \"metadata\": {\"version\": 1},\n",
    "            }\n",
    "            product_id = file.replace(\".yml\", \"\").replace(s3_path, \"\")\n",
    "            master[\"sources\"][product_id] = product_dict\n",
    "\n",
    "    # Write the updated master catalog back to the file\n",
    "    write_yaml(master_cat_path, master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec5b82-ee23-4093-890b-de2d1ec875a7",
   "metadata": {},
   "source": [
    "___\n",
    "## **2. Showcasing how the functions works**\n",
    "___\n",
    "#### 2.1 Creating a catalog for a product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9510b-e1aa-47fd-b935-5e9bd2b04c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_product_yml(\n",
    "    product_id=\"GLOBAL_ANALYSISFORECAST_PHY_001_024\",\n",
    "    storage_location=\"copernicus_catalogs/product_catalogs/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90a134-e5ac-4401-ab36-a6fc89b26a8b",
   "metadata": {},
   "source": [
    "___\n",
    "The function **create_product_yml** creates a catalaog with the product ID that you can find on the product page, in the classification section. It stores it in the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6bb2c-844a-4f14-964a-5e6c0cab54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_master_cat(\n",
    "    master_cat_path=\"copernicus_catalogs/master.yml\",\n",
    "    copernicus_cat_path=\"copernicus_catalogs/product_catalogs/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc0000-1939-496c-bbd5-f39957d1dcbf",
   "metadata": {},
   "source": [
    "The function **update_master_cat** creates masetr catalog if no catalog already exists. Then it loops over the catalogs in the given folder to access them and create an entry in the sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d40bd-3560-445d-8a9c-fae4a95c51e8",
   "metadata": {},
   "source": [
    "#### 2.2 How to open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097db09-0a2f-4bfd-9ae6-e8794319f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the master catalog\n",
    "cat = intake.open_catalog(\"copernicus_catalogs/master.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca204c00-4993-4c14-a5ab-1392d6f86d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can acces the product we just created the following way\n",
    "cat.GLOBAL_ANALYSISFORECAST_PHY_001_024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b09403-4523-4163-abd6-89f93953fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "product = cat.GLOBAL_ANALYSISFORECAST_PHY_001_024\n",
    "product[\"cmems_mod_glo_phy-thetao_anfc_0.083deg_P1D-m\"](chunk=\"time\").to_dask()\n",
    "\n",
    "# Don't forget to specify the chunking type or it won't work and this error won't be shown explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfaf2ac-74dd-4e2d-a0b1-3f974cc6613a",
   "metadata": {},
   "source": [
    "Let's add another source to our master catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9b9f8-f6b5-4091-824e-ee80f8004788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new source and store it in the same folder\n",
    "create_product_yml(\n",
    "    product_id=\"IBI_MULTIYEAR_PHY_005_002\",\n",
    "    storage_location=\"copernicus_catalogs/product_catalogs/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727a779-e296-4042-9776-fc3afa58b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updataing the catalog\n",
    "update_master_cat(\n",
    "    master_cat_path=\"copernicus_catalogs/master.yml\",\n",
    "    copernicus_cat_path=\"copernicus_catalogs/product_catalogs/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0f71c-41b6-4326-9072-623439bb3291",
   "metadata": {},
   "source": [
    "Let's have a look at our new product :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cdd800-60db-4e13-be62-3fcfb1841f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the master catalog\n",
    "cat = intake.open_catalog(\"copernicus_catalogs/master.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a77ff-9d1b-4f89-bec9-db0dba5e5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that we have two products now\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca67da3-9412-4447-9a98-c39f8b828e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = cat.IBI_MULTIYEAR_PHY_005_002\n",
    "product[\"cmems_mod_ibi_phy_my_0.083deg-2D_PT1H-m\"](chunk=\"time\").to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c41816-820a-4597-ada6-a003869af993",
   "metadata": {},
   "source": [
    "### **3. Exploring and selecting a variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3396a602-e1e9-4103-823e-7d857d9773b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating catalogs to explore\n",
    "create_product_yml(\"GLOBAL_ANALYSISFORECAST_PHY_001_024\")\n",
    "create_product_yml(\"IBI_MULTIYEAR_PHY_005_002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6153b-926a-470d-842d-4e776e5c9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_selector(master_cat, target_variable, product_id=None):\n",
    "    \"\"\"\n",
    "    Select datasets containing a specific target variable from a given catalog.\n",
    "\n",
    "    Parameters:\n",
    "    master_cat (str): Path to the master catalog file.\n",
    "    target_variable (str): The variable to search for within the datasets.\n",
    "                           Valid values are \"thetao\", \"deptho\", and \"zos\".\n",
    "    product_id (str, optional): The specific product ID to filter by. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    list or dict:\n",
    "        - If product_id is provided, returns a list of dataset IDs within the specified product\n",
    "          that contain the target variable.\n",
    "        - If product_id is not provided, returns a dictionary where keys are product IDs and\n",
    "          values are lists of dataset IDs within each product that contain the target variable.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the target_variable is not one of the authorized values.\n",
    "    \"\"\"\n",
    "    if target_variable not in [\"thetao\", \"deptho\", \"zos\"]:\n",
    "        raise ValueError(\n",
    "            f\"Wrong target variable: {target_variable}. Authorized values are: ['thetao', 'deptho', 'zos']\"\n",
    "        )\n",
    "\n",
    "    if product_id is not None:\n",
    "        target_products = []\n",
    "        cat = intake.open_catalog(master_cat)[product_id]\n",
    "        possible_datasets = list(cat)\n",
    "        for dataset_id in possible_datasets:\n",
    "            if target_variable in cat[dataset_id].metadata[\"variable\"].keys():\n",
    "                target_products.append(dataset_id)\n",
    "\n",
    "    else:\n",
    "        target_products = {}\n",
    "        cat = intake.open_catalog(\"copernicus_catalogs/master.yml\")\n",
    "        for product in list(cat):\n",
    "            target_list = []\n",
    "            sub_cat = intake.open_catalog(master_cat)[product]\n",
    "            possible_datasets = list(sub_cat)\n",
    "            for dataset_id in possible_datasets:\n",
    "                if target_variable in sub_cat[dataset_id].metadata[\"variable\"].keys():\n",
    "                    target_list.append(dataset_id)\n",
    "            target_products[product] = target_list\n",
    "    return target_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253578ed-c44f-4293-89c9-b6325dbbd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showcase the datasets were the variable is available\n",
    "variable_selector(master_cat=\"copernicus_catalogs/master.yml\", target_variable=\"zos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5406f-86d9-4c09-aad0-7abba43cbae5",
   "metadata": {},
   "source": [
    "### **4. Saving to the bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d1ed4-c706-4baf-bf8a-a683e0fb7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the product catalogs in at this path in the bucket : gfts-ifremer/copernicus_catalogs/product_catalogs\n",
    "s3.put(\n",
    "    \"copernicus_catalogs/product_catalogs/\",\n",
    "    \"gfts-ifremer/copernicus_catalogs/product_catalogs\",\n",
    "    recursive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4070017-f84f-4121-8604-f14368d708ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the files to see waht's inside\n",
    "s3.ls(\"gfts-ifremer/copernicus_catalogs\", refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d2957-d314-448c-a827-5dde32c38238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function shapes the files for the remote storage of data for s3 bucket storage\n",
    "update_master_cat_remote(\"copernicus_catalogs/master.yml\", s3=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b318a-2671-4da9-b07a-c1ec1b8168c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the master catalog with the adapted remote paths and not local files\n",
    "s3.put(\n",
    "    \"copernicus_catalogs/master.yml\",\n",
    "    \"gfts-ifremer/copernicus_catalogs/master.yml\",\n",
    "    recursive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b507d4b-a270-484e-8950-9ce538f7dbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
